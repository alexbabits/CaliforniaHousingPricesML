{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install scikit-learn \n",
    "%pip install pandas\n",
    "%pip install xgboost\n",
    "%pip install optuna\n",
    "%pip install lightgbm\n",
    "%pip install matplotlib\n",
    "%pip install catboost\n",
    "%pip install plotly\n",
    "%pip install haversine\n",
    "%pip install umap-learn\n",
    "%pip install reverse_geocoder\n",
    "%pip install shapely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna as op\n",
    "import plotly.express as px\n",
    "import plotly.subplots as sp\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from umap import UMAP\n",
    "from sklearn.cluster import KMeans\n",
    "from haversine import haversine\n",
    "import reverse_geocoder as rg\n",
    "from shapely.geometry import LineString, Point\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict, KFold\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gathering and combining training and testing data (But keeping the target column on 'y' for training.) \n",
    "training_df = pd.read_csv('./train.csv', index_col='id')\n",
    "testing_df = pd.read_csv('./test.csv', index_col='id')\n",
    "# Drop the three outlier rows from the training set\n",
    "to_drop = [20991, 34363, 33228]\n",
    "training_df = training_df.drop(to_drop)\n",
    "# Split the target variable from the training set\n",
    "y = training_df.MedHouseVal\n",
    "training_df = training_df.drop(['MedHouseVal'], axis=1)\n",
    "# Concatenate the training and testing sets\n",
    "X = pd.concat([training_df, testing_df], axis=0, ignore_index=True)\n",
    "\n",
    "# X is both training and testing data! Will unmerge them after feature engineering, so I can tune, train, and fit the models on the training data only!\n",
    "# This was done so I could easily feature engineer both the training and testing data at the same time."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                                            Feature Engineering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning:\n",
      "\n",
      "The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "\n"
     ]
    }
   ],
   "source": [
    "coordinates = X[['Latitude', 'Longitude']].values\n",
    "clustering = KMeans(n_clusters=20, max_iter=500, random_state=42).fit(coordinates)\n",
    "cluster_centers = {i: tuple(centroid) for i, centroid in enumerate(clustering.cluster_centers_)}\n",
    "\n",
    "def cluster_features(df):\n",
    "    for i, cc in enumerate(cluster_centers.values()):\n",
    "        df[f'cluster_{i}'] = df.apply(lambda x: haversine((x['Latitude'], x['Longitude']), cc, unit='ft'), axis=1)\n",
    "    return df\n",
    "\n",
    "X = cluster_features(X)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coordinates with PCA and UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA().fit(coordinates)\n",
    "X['pca_lat'] = pca.transform(coordinates)[:,0]\n",
    "X['pca_lon'] = pca.transform(coordinates)[:,1]\n",
    "\n",
    "umap = UMAP(n_components=2, \n",
    "            n_neighbors=50, \n",
    "            random_state=42).fit(coordinates)\n",
    "X['umap_lat'] = umap.transform(coordinates)[:,0]\n",
    "X['umap_lon'] = umap.transform(coordinates)[:,1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cartesian coordinates rotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['rot_15_x'] = (np.cos(np.radians(15)) * X['Longitude']) + \\\n",
    "                  (np.sin(np.radians(15)) * X['Latitude'])\n",
    "    \n",
    "X['rot_15_y'] = (np.cos(np.radians(15)) * X['Latitude']) + \\\n",
    "                  (np.sin(np.radians(15)) * X['Longitude'])\n",
    "    \n",
    "X['rot_30_x'] = (np.cos(np.radians(30)) * X['Longitude']) + \\\n",
    "                  (np.sin(np.radians(30)) * X['Latitude'])\n",
    "    \n",
    "X['rot_30_y'] = (np.cos(np.radians(30)) * X['Latitude']) + \\\n",
    "                  (np.sin(np.radians(30)) * X['Longitude'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding location of cities (and coastline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting county info for each location based on its coordinates, and encoding that info as a numerical value in 'X'.\n",
    "coordinates = list(zip(X['Latitude'], X['Longitude']))\n",
    "results = rg.search(coordinates)\n",
    "X['place'] = [x['admin2'] for x in results]\n",
    "\n",
    "places = ['Los Angeles County', 'Orange County', 'Kern County',\n",
    "          'Alameda County', 'San Francisco County', 'Ventura County',\n",
    "          'Santa Clara County', 'Fresno County', 'Santa Barbara County',\n",
    "          'Contra Costa County', 'Yolo County', 'Monterey County',\n",
    "          'Riverside County', 'Napa County']\n",
    "\n",
    "def replace(x):\n",
    "    if x in places:\n",
    "        return x\n",
    "    else:\n",
    "        return 'Other'\n",
    "    \n",
    "X['place'] = X['place'].apply(lambda x: replace(x))\n",
    "le = LabelEncoder()\n",
    "X['place'] = le.fit_transform(X['place'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the distance to cities and coast points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually gathered location of major cities\n",
    "SC = (38.576931, -121.494949)\n",
    "SF = (37.780080, -122.420160)\n",
    "SJ = (37.334789, -121.888138)\n",
    "LA = (34.052235, -118.243683)\n",
    "SD = (32.715759, -117.163818)\n",
    "# Finds distance from coordinates in dataframe from cities.\n",
    "X['dist_SC'] = X.apply(lambda x: haversine((x['Latitude'], x['Longitude']), SC, unit='ft'), axis=1)\n",
    "X['dist_SF'] = X.apply(lambda x: haversine((x['Latitude'], x['Longitude']), SF, unit='ft'), axis=1)\n",
    "X['dist_SJ'] = X.apply(lambda x: haversine((x['Latitude'], x['Longitude']), SJ, unit='ft'), axis=1)\n",
    "X['dist_LA'] = X.apply(lambda x: haversine((x['Latitude'], x['Longitude']), LA, unit='ft'), axis=1)\n",
    "X['dist_SD'] = X.apply(lambda x: haversine((x['Latitude'], x['Longitude']), SD, unit='ft'), axis=1)\n",
    "X['dist_nearest_city'] = X[['dist_SC', 'dist_SF', 'dist_SJ', \n",
    "                              'dist_LA', 'dist_SD']].min(axis=1)\n",
    "\n",
    "# Manually gathered location of coastline\n",
    "coast_points = LineString([(32.664, -117.161), (33.206, -117.383),\n",
    "                           (33.777, -118.202), (34.463, -120.014),\n",
    "                           (35.427, -120.881), (35.928, -121.489),\n",
    "                           (36.982, -122.028), (37.611, -122.491),\n",
    "                           (38.355, -123.060), (39.792, -123.821),\n",
    "                           (40.799, -124.188), (41.755, -124.197)])\n",
    "# Finds shortest distance to coast from coordinates\n",
    "X['dist_to_coast'] = X.apply(lambda x: Point(x['Latitude'], x['Longitude']).distance(coast_points), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting df back\n",
    "n_train = len(training_df)\n",
    "training_df = X.iloc[:n_train, :]\n",
    "testing_df = X.iloc[n_train:, :]\n",
    "y = y.reset_index(drop=True)\n",
    "testing_df = testing_df.reset_index(drop=True)\n",
    "# Fix the index of testing_df to start from 37137\n",
    "testing_df.index += 37137"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                                        Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to tune hyperparameters of LGBM model\n",
    "class LGBMObjective:\n",
    "    def __init__(self, model, X, y):\n",
    "        self.model = model\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    \n",
    "    def __call__(self, trial):\n",
    "        # Define the search space for the hyperparameters\n",
    "        n_estimators = trial.suggest_int('n_estimators', 500, 1400)\n",
    "        max_depth = trial.suggest_int('max_depth', 7, 9)\n",
    "        learning_rate = trial.suggest_float('learning_rate', 0.018, 0.038, log=True)\n",
    "        num_leaves = trial.suggest_int('num_leaves', 60, 180)\n",
    "        min_child_samples = trial.suggest_int('min_child_samples', 70, 140)\n",
    "\n",
    "        # Set the hyperparameters in the model\n",
    "        self.model.set_params(\n",
    "            n_estimators=n_estimators,\n",
    "            max_depth=max_depth,\n",
    "            learning_rate=learning_rate,\n",
    "            num_leaves=num_leaves,\n",
    "            min_child_samples=min_child_samples\n",
    "        )\n",
    "\n",
    "        # Calculate the root mean squared error (RMSE) using cross-validation\n",
    "        cv = KFold(n_splits=8, shuffle=True, random_state=42)\n",
    "        rmse_scores = np.sqrt(-1 * cross_val_score(self.model, self.X, self.y, cv=cv, scoring='neg_mean_squared_error', n_jobs=1))\n",
    "        mean_rmse = rmse_scores.mean()\n",
    "\n",
    "        return mean_rmse\n",
    "\n",
    "lgbm = LGBMRegressor(n_jobs=2)\n",
    "obj = LGBMObjective(lgbm, training_df, y)\n",
    "\n",
    "# Create the Optuna study and optimize the hyperparameters\n",
    "study = op.create_study(direction='minimize')\n",
    "study.optimize(obj, n_trials=15)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "best_params = study.best_params\n",
    "print(f\"Best hyperparameters: {best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to tune hyperparameters of LGBM model\n",
    "class CatBoostObjective:\n",
    "    def __init__(self, model, X, y):\n",
    "        self.model = model\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    \n",
    "    def __call__(self, trial):\n",
    "        # Define the search space for the hyperparameters\n",
    "        iterations = trial.suggest_int('iterations', 20, 100)\n",
    "        depth = trial.suggest_int('depth', 6, 10)\n",
    "        #learning_rate = trial.suggest_int('learning_rate', 0.1, 1)\n",
    "\n",
    "        # Set the hyperparameters in the model\n",
    "        self.model.set_params(\n",
    "            iterations=iterations,\n",
    "            depth=depth\n",
    "            #learning_rate=learning_rate\n",
    "        )\n",
    "\n",
    "        # Calculate the root mean squared error (RMSE) using cross-validation\n",
    "        cv = KFold(n_splits=8, shuffle=True, random_state=42)\n",
    "        rmse_scores = np.sqrt(-1 * cross_val_score(self.model, self.X, self.y, cv=cv, scoring='neg_mean_squared_error', n_jobs=1))\n",
    "        mean_rmse = rmse_scores.mean()\n",
    "\n",
    "        return mean_rmse\n",
    "\n",
    "catB = CatBoostRegressor(thread_count=4)\n",
    "obj = CatBoostObjective(catB, training_df, y)\n",
    "\n",
    "# Create the Optuna study and optimize the hyperparameters\n",
    "study = op.create_study(direction='minimize')\n",
    "study.optimize(obj, n_trials=15)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "best_params = study.best_params\n",
    "print(f\"Best hyperparameters: {best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'LGBM Regressor': LGBMRegressor(n_estimators=800, max_depth=7, learning_rate=0.028, num_leaves=100, min_child_samples=100),\n",
    "    'XGBoost': XGBRegressor(n_estimators=400, max_depth=6, learning_rate=0.01),\n",
    "    'CatBoost': CatBoostRegressor(iterations=72, depth=6, learning_rate=0.475, thread_count=4)\n",
    "}\n",
    "\n",
    "# Split your data into training and validation sets\n",
    "train_X, validation_X, train_y, validation_y = train_test_split(training_df, y, train_size=0.8, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Individual Base Model predictions\n",
    "for model_name, model in models.items():\n",
    "    cv_scores = cross_val_score(model, train_X, train_y, cv=8, scoring='neg_root_mean_squared_error')\n",
    "    mean_rmse = -np.mean(cv_scores)\n",
    "    print(f\"RMSE for the {model_name} model with cross-validation: {mean_rmse}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                                                Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit each of your base models on the full training data.\n",
    "for model_name, model in models.items():\n",
    "    model.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions using the base models on the validation set\n",
    "base_model_predictions = []\n",
    "for model_name, model in models.items():\n",
    "    prediction = model.predict(validation_X)\n",
    "    base_model_predictions.append(prediction)\n",
    "\n",
    "# Stack predictions horizontally (i.e., column-wise)\n",
    "stacked_predictions = np.column_stack(base_model_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LGBMRegressor()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LGBMRegressor</label><div class=\"sk-toggleable__content\"><pre>LGBMRegressor()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LGBMRegressor()"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train a meta-model using the stacked predictions as input and the true validation_y values as output. linear regressor as the meta-model is fine.\n",
    "meta_model = LGBMRegressor()\n",
    "meta_model.fit(stacked_predictions, validation_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta model RMSE on validation data: 0.5126655757952071\n"
     ]
    }
   ],
   "source": [
    "# Make predictions using the meta-model on the validation set\n",
    "validation_meta_model_predictions = meta_model.predict(stacked_predictions)\n",
    "\n",
    "# Calculate the RMSE\n",
    "validation_rmse = np.sqrt(mean_squared_error(validation_y, validation_meta_model_predictions))\n",
    "print(f\"Meta model RMSE on validation data: {validation_rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions using the base models on the test set\n",
    "test_base_model_predictions = []\n",
    "for model_name, model in models.items():\n",
    "    test_prediction = model.predict(testing_df)\n",
    "    test_base_model_predictions.append(test_prediction)\n",
    "\n",
    "# Stack predictions horizontally (i.e., column-wise)\n",
    "test_stacked_predictions = np.column_stack(test_base_model_predictions)\n",
    "\n",
    "# Make predictions using the meta-model on the test set\n",
    "test_meta_model_predictions = meta_model.predict(test_stacked_predictions)\n",
    "\n",
    "# Create a DataFrame for the submission\n",
    "submission = pd.DataFrame({\n",
    "    'id': testing_df.index,\n",
    "    'MedHouseVal': test_meta_model_predictions\n",
    "})\n",
    "\n",
    "# Save the submission DataFrame to a CSV file\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(submission)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                                    EDA & initial data visualization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heatmap of MedHouseVal based on location (from initial training data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = px.scatter(training_df, x='Latitude', y='Longitude', color='MedHouseVal',\n",
    "                 color_continuous_scale='Jet', size_max=25, opacity=1,\n",
    "                 title='Target Distribution', width=800, height=800)\n",
    "\n",
    "fig.update_xaxes(title_text='Latitude', showgrid=False, showline=True)\n",
    "fig.update_yaxes(title_text='Longitude', showgrid=False, showline=True)\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Displays feature values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a subplot grid with 3 columns and 3 rows.\n",
    "fig = sp.make_subplots(rows=3, cols=3, subplot_titles=training_df.columns, horizontal_spacing=0.05, vertical_spacing=0.1)\n",
    "# Loop through the columns and create a histogram for each feature.\n",
    "for i, col in enumerate(training_df.columns):\n",
    "    row = i // 3 + 1\n",
    "    col_idx = i % 3 + 1\n",
    "    x = training_df[col]\n",
    "    if col in ['AveBedrms', 'Population', 'AveOccup', 'AveRooms']:\n",
    "        x = np.log(x)\n",
    "    fig.add_trace(go.Histogram(x=x, nbinsx=15, marker=dict(color='#000080')), row=row, col=col_idx)\n",
    "\n",
    "fig.update_layout(title_text='Features Distribution', title_x=0.5, showlegend=False, width=1000, height=800, plot_bgcolor='#D3D3D3', paper_bgcolor='#FFFDD0',\n",
    "                  font=dict(family='Arial', color='#000080'))\n",
    "fig.update_xaxes(showgrid=False)\n",
    "fig.update_yaxes(showgrid=False, showticklabels=False)\n",
    "fig.show()\n",
    "\n",
    "for column in training_df.columns:\n",
    "    print(f\"Statistics for {column}:\")\n",
    "    print(f\"Min: {training_df[column].min()}\")\n",
    "    print(f\"Max: {training_df[column].max()}\")\n",
    "    print(f\"Mean: {training_df[column].mean()}\")\n",
    "    print(f\"Std: {training_df[column].std()}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot of the PCA and UMAP features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine 'X' and 'y' into a single DataFrame\n",
    "data_for_plt = X.copy()\n",
    "data_for_plt['MedHouseVal'] = y\n",
    "\n",
    "# Create a subplot grid with 1 row and 2 columns\n",
    "fig = sp.make_subplots(rows=1, cols=2, subplot_titles=['PCA', 'UMAP'], horizontal_spacing=0.1)\n",
    "\n",
    "# Create the PCA scatter plot\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=data_for_plt['pca_lat'], y=data_for_plt['pca_lon'], mode='markers',\n",
    "               marker=dict(size=6, color=data_for_plt['MedHouseVal'], colorscale='Jet', opacity=1), showlegend=False),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Create the UMAP scatter plot\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=data_for_plt['umap_lat'], y=data_for_plt['umap_lon'], mode='markers',\n",
    "               marker=dict(size=6, color=data_for_plt['MedHouseVal'], colorscale='Jet', opacity=1), showlegend=False),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# Update the layout of the subplot grid\n",
    "fig.update_layout(title_text='PCA and UMAP', title_x=0.5, width=1200, height=600)\n",
    "fig.update_xaxes(showgrid=False)\n",
    "fig.update_yaxes(showgrid=False)\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot of the Cartesian coordinates rotated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine 'X' and 'y' into a single DataFrame\n",
    "data_for_plt = X.copy()\n",
    "data_for_plt['MedHouseVal'] = y\n",
    "\n",
    "# Create a subplot grid with 1 row and 2 columns\n",
    "fig = sp.make_subplots(rows=1, cols=2, subplot_titles=['15 degrees rotation', '30 degrees rotation'], horizontal_spacing=0.1)\n",
    "\n",
    "# Create the 15 degrees rotation scatter plot\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=data_for_plt['rot_15_x'], y=data_for_plt['rot_15_y'], mode='markers',\n",
    "               marker=dict(size=6, color=data_for_plt['MedHouseVal'], colorscale='Jet', opacity=0.5), showlegend=False),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Create the 30 degrees rotation scatter plot\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=data_for_plt['rot_30_x'], y=data_for_plt['rot_30_y'], mode='markers',\n",
    "               marker=dict(size=6, color=data_for_plt['MedHouseVal'], colorscale='Jet', opacity=0.5), showlegend=False),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# Update the layout of the subplot grid\n",
    "fig.update_layout(title_text='Cartesian Coordinates Rotation', title_x=0.5, width=1200, height=600)\n",
    "fig.update_xaxes(showgrid=False)\n",
    "fig.update_yaxes(showgrid=False)\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot of feature importances for LGBM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_model = models['LGBM Regressor']\n",
    "\n",
    "# Get feature importances from the LGBM model\n",
    "feature_importances = lgbm_model.feature_importances_\n",
    "\n",
    "# Create a DataFrame with feature names and their importance values\n",
    "feature_importance_df = pd.DataFrame({'feature': training_df.columns, 'importance': feature_importances})\n",
    "\n",
    "# Sort the DataFrame by importance values in descending order\n",
    "feature_importance_df = feature_importance_df.sort_values('importance', ascending=False)\n",
    "\n",
    "# Create a Plotly bar chart\n",
    "fig = go.Figure(go.Bar(x=feature_importance_df['importance'], y=feature_importance_df['feature'], orientation='h'))\n",
    "\n",
    "# Customize the chart's appearance\n",
    "fig.update_layout(\n",
    "    title='Feature Importances',\n",
    "    xaxis_title='Importance',\n",
    "    yaxis_title='Feature',\n",
    "    font=dict(\n",
    "        family=\"Calibri\",\n",
    "        size=14,\n",
    "        color=\"#444444\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Show the chart\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
